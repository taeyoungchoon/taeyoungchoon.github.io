* log

https://stafwag.github.io/blog/blog/2019/01/21/settinp-up-openstack-ansible-all-in-one-on-a-centos-7-system/

[root@c71 openstack-ansible]# ./scripts/bootstrap-aio.sh

TASK [bootstrap-host : Fail if there is not enough space available in /] 
fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "msg": "Not enough space available in /.\nFound 36.39 GB, required 50 GB)\n"}

# pwd
/opt/openstack-ansible/tests/roles/bootstrap-host/tasks
# ls check-requirements.yml
check-requirements.yml
#

- name: Fail if there is not enough space available in /
  fail:
    msg: |
      Not enough space available in /.
      Found {{ root_gb_available }} GB, required {{ bootstrap_host_data_disk_min_size }} GB)
  when:
    - bootstrap_host_data_disk_device == None
    - (host_root_space_available_bytes | int) < (host_data_disk_min_size_bytes | int)
  tags:
    - check-disk-size

# grep 50 tests/roles/bootstrap-host/defaults/main.yml
bootstrap_host_data_disk_min_size: 50
#

# grep bootstrap_host_data_disk_min_size tests/roles/bootstrap-host/defaults/main.yml
bootstrap_host_data_disk_min_size: 10
#

[root@c71 openstack-ansible]# ip -4 -o a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
2: eth0    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0\       valid_lft 84515sec preferred_lft 84515sec
3: br-mgmt    inet 172.29.236.100/22 brd 172.29.239.255 scope global br-mgmt\       valid_lft forever preferred_lft forever
4: br-storage    inet 172.29.244.100/22 brd 172.29.247.255 scope global br-storage\       valid_lft forever preferred_lft forever
5: br-vlan    inet 172.29.248.100/22 brd 172.29.251.255 scope global br-vlan\       valid_lft forever preferred_lft forever
5: br-vlan    inet 172.29.248.1/22 brd 172.29.251.255 scope global secondary br-vlan:0\       valid_lft forever preferred_lft forever
8: br-vxlan    inet 172.29.240.100/22 brd 172.29.243.255 scope global br-vxlan\       valid_lft forever preferred_lft forever

# ls ifcfg-*
ifcfg-br-mgmt  ifcfg-br-storage  ifcfg-br-vlan  ifcfg-br-vlan:0  ifcfg-br-vxlan  ifcfg-eth0  ifcfg-lo
#

# head -1000 ifcfg-*
==> ifcfg-br-mgmt <==
DEVICE=br-mgmt
TYPE=Bridge
IPADDR=172.29.236.100
NETMASK=255.255.252.0
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
DELAY=0
ETHTOOL_OPTS="-K ${DEVICE} sg off"

==> ifcfg-br-storage <==
DEVICE=br-storage
TYPE=Bridge
IPADDR=172.29.244.100
NETMASK=255.255.252.0
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
DELAY=0
ETHTOOL_OPTS="-K ${DEVICE} sg off"

==> ifcfg-br-vlan <==
# This interface has a veth peer
DEVICE=br-vlan
TYPE=Bridge
IPADDR=172.29.248.100
NETMASK=255.255.252.0
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
DELAY=0
ETHTOOL_OPTS="-K ${DEVICE} sg off"

==> ifcfg-br-vlan:0 <==
# This interface is an alias
DEVICE=br-vlan:0
IPADDR=172.29.248.1
NETMASK=255.255.252.0
ONBOOT=yes

==> ifcfg-br-vxlan <==
DEVICE=br-vxlan
TYPE=Bridge
IPADDR=172.29.240.100
NETMASK=255.255.252.0
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
DELAY=0
ETHTOOL_OPTS="-K ${DEVICE} sg off"

==> ifcfg-eth0 <==
DEVICE="eth0"
BOOTPROTO="dhcp"
ONBOOT="yes"
TYPE="Ethernet"
PERSISTENT_DHCLIENT="yes"

==> ifcfg-lo <==
DEVICE=lo
IPADDR=127.0.0.1
NETMASK=255.0.0.0
NETWORK=127.0.0.0
# If you're having problems with gated making 127.0.0.0/8 a martian,
# you can change this to something else (255.255.255.255, for example)
BROADCAST=127.255.255.255
ONBOOT=yes
NAME=loopback
#

# brctl show
bridge name	bridge id		STP enabled	interfaces
br-mgmt		8000.000000000000	no
br-storage		8000.000000000000	no
br-vlan		8000.c22fbc05933f	no		br-vlan-veth
br-vxlan		8000.000000000000	no
#


# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:8a:fe:e6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0
       valid_lft 84157sec preferred_lft 84157sec
    inet6 fe80::5054:ff:fe8a:fee6/64 scope link
       valid_lft forever preferred_lft forever
3: br-mgmt: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 36:da:e5:74:27:43 brd ff:ff:ff:ff:ff:ff
    inet 172.29.236.100/22 brd 172.29.239.255 scope global br-mgmt
       valid_lft forever preferred_lft forever
    inet6 fe80::34da:e5ff:fe74:2743/64 scope link
       valid_lft forever preferred_lft forever
4: br-storage: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 2a:3f:84:ad:8e:ca brd ff:ff:ff:ff:ff:ff
    inet 172.29.244.100/22 brd 172.29.247.255 scope global br-storage
       valid_lft forever preferred_lft forever
    inet6 fe80::283f:84ff:fead:8eca/64 scope link
       valid_lft forever preferred_lft forever
5: br-vlan: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether c2:2f:bc:05:93:3f brd ff:ff:ff:ff:ff:ff
    inet 172.29.248.100/22 brd 172.29.251.255 scope global br-vlan
       valid_lft forever preferred_lft forever
    inet 172.29.248.1/22 brd 172.29.251.255 scope global secondary br-vlan:0
       valid_lft forever preferred_lft forever
    inet6 fe80::eccd:2aff:fe3f:668/64 scope link
       valid_lft forever preferred_lft forever
6: eth12@br-vlan-veth: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 12:4c:ad:0b:08:a3 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::104c:adff:fe0b:8a3/64 scope link
       valid_lft forever preferred_lft forever
7: br-vlan-veth@eth12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br-vlan state UP group default qlen 1000
    link/ether c2:2f:bc:05:93:3f brd ff:ff:ff:ff:ff:ff
    inet6 fe80::c02f:bcff:fe05:933f/64 scope link
       valid_lft forever preferred_lft forever
8: br-vxlan: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 32:74:c4:49:b1:96 brd ff:ff:ff:ff:ff:ff
    inet 172.29.240.100/22 brd 172.29.243.255 scope global br-vxlan
       valid_lft forever preferred_lft forever
    inet6 fe80::3074:c4ff:fe49:b196/64 scope link
       valid_lft forever preferred_lft forever
#

# ovs-vsctl
-bash: ovs-vsctl: command not found
#

# bridge link
7: br-vlan-veth state UP @eth12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master br-vlan state forwarding priority 32 cost 2
#

# ip a | grep veth
6: eth12@br-vlan-veth: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
7: br-vlan-veth@eth12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br-vlan state UP group default qlen 1000
#

# ip a s eth12
6: eth12@br-vlan-veth: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 12:4c:ad:0b:08:a3 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::104c:adff:fe0b:8a3/64 scope link
       valid_lft forever preferred_lft forever
#

# pwd
/opt/openstack-ansible/playbooks
#

# pwd
/etc/openstack_deploy
# ls openstack_user_config.yml user_variables.yml
openstack_user_config.yml  user_variables.yml
#

# pstree
systemd─┬─NetworkManager─┬─dhclient
        │                └─2*[{NetworkManager}]
        ├─agetty
        ├─anacron
        ├─auditd───{auditd}
        ├─chronyd
        ├─crond
        ├─dbus-daemon───{dbus-daemon}
        ├─dnsmasq
        ├─gssproxy───5*[{gssproxy}]
        ├─lvmetad
        ├─master─┬─pickup
        │        └─qmgr
        ├─polkitd───6*[{polkitd}]
        ├─rpcbind
        ├─rsyslogd───2*[{rsyslogd}]
        ├─ssh
        ├─sshd───sshd───bash───sudo───bash───bash───ansible-playboo─┬─ansible-playboo───ssh
        │                                                           └─2*[{ansible-playboo}]
        ├─sshd───sh───python───python───gtar───xz
        ├─sshd───sshd───bash───sudo───bash───pstree
        ├─sshd
        ├─systemd-journal
        ├─systemd-logind
        ├─systemd-machine
        ├─systemd-udevd
        └─tuned───4*[{tuned}]
#

TASK [lxc_hosts : Ensure that the LXC cache has been prepared] *********************************************************************************************************************************************
Thursday 18 July 2019  13:18:01 +0000 (0:00:00.786)       0:11:04.241 *********
FAILED - RETRYING: Ensure that the LXC cache has been prepared (120 retries left).
FAILED - RETRYING: Ensure that the LXC cache has been prepared (119 retries left).
FAILED - RETRYING: Ensure that the LXC cache has been prepared (118 retries left).
changed: [aio1]

ASK [lxc_container_create : Create container (dir)]

# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4 IPV6
aio1_horizon_container-cb145a57        STOPPED 1         onboot, openstack -    -
aio1_keystone_container-1753b49d       STOPPED 1         onboot, openstack -    -
aio1_neutron_server_container-75989006 STOPPED 1         onboot, openstack -    -
aio1_swift_proxy_container-6f69669c    STOPPED 1         onboot, openstack -    -
aio1_utility_container-8d3db5a0        STOPPED 1         onboot, openstack -    -
#

# lxc-info --name aio1_horizon_container-cb145a57
Name:           aio1_horizon_container-cb145a57
State:          RUNNING
PID:            22361
CPU use:        0.75 seconds
BlkIO use:      72.71 MiB
Memory use:     8.99 MiB
KMem use:       0 bytes
Link:           vethN938XE
 TX bytes:      656 bytes
 RX bytes:      3.20 KiB
 Total bytes:   3.84 KiB
#

# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4 IPV6
aio1_cinder_api_container-ade654c3     STOPPED 1         onboot, openstack -    -
aio1_designate_container-1a360417      STOPPED 1         onboot, openstack -    -
aio1_glance_container-03078ce1         STOPPED 1         onboot, openstack -    -
aio1_horizon_container-cb145a57        RUNNING 1         onboot, openstack -    -
aio1_keystone_container-1753b49d       RUNNING 1         onboot, openstack -    -
aio1_memcached_container-4cb65778      STOPPED 1         onboot, openstack -    -
aio1_neutron_server_container-75989006 RUNNING 1         onboot, openstack -    -
aio1_repo_container-e846b452           STOPPED 1         onboot, openstack -    -
aio1_swift_proxy_container-6f69669c    RUNNING 1         onboot, openstack -    -
aio1_utility_container-8d3db5a0        RUNNING 1         onboot, openstack -    -
#

 free -m
              total        used        free      shared  buff/cache   available
Mem:            487         289           5          20         192         122
Swap:

# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1        40G   11G   30G  27% /
devtmpfs        237M     0  237M   0% /dev
tmpfs           244M  324K  244M   1% /dev/shm
tmpfs           244M  680K  243M   1% /run
tmpfs           244M     0  244M   0% /sys/fs/cgroup
tmpfs            49M     0   49M   0% /run/user/1000
/dev/loop0      128G  465M  126G   1% /var/lib/machines
/dev/loop2     1008G   77M  957G   1% /var/lib/nova/instances
/dev/loop3      1.0T   33M  1.0T   1% /srv/swift1.img
/dev/loop4      1.0T   33M  1.0T   1% /srv/swift2.img
/dev/loop5      1.0T   33M  1.0T   1% /srv/swift3.img
tmpfs            49M     0   49M   0% /run/user/0
#

? df -h
Filesystem      Size   Used  Avail Capacity iused               ifree %iused  Mounted on
/dev/disk1s1    70Gi   65Gi  2.0Gi    98%  868105 9223372036853907702    0%   /
devfs          191Ki  191Ki    0Bi   100%     662                   0  100%   /dev
/dev/disk1s4    70Gi  2.0Gi  2.0Gi    51%       2 9223372036854775805    0%   /private/var/vm
/dev/disk0s3   9.2Gi  206Mi  9.0Gi     3%    6576              296048    2%   /Volumes/SHARE
map -hosts       0Bi    0Bi    0Bi   100%       0                   0  100%   /net
map auto_home    0Bi    0Bi    0Bi   100%       0                   0  100%   /home
/dev/disk0s4    33Gi   27Gi  6.5Gi    81%  273912             6925788    4%   /Volumes/BOOTCAMP
?

? df -Pk
Filesystem    1024-blocks     Used Available Capacity  Mounted on
/dev/disk1s1     73204200 61325516   8594084    88%    /
devfs                 191      191         0   100%    /dev
/dev/disk1s4     73204200  2097504   8594084    20%    /private/var/vm
/dev/disk0s3      9683968   210432   9473536     3%    /Volumes/SHARE
map -hosts              0        0         0   100%    /net
map auto_home           0        0         0   100%    /home
/dev/disk0s4     34756240 27915708   6840532    81%    /Volumes/BOOTCAMP
?

# free -m
              total        used        free      shared  buff/cache   available
Mem:            487         260          24          22         201         143
Swap:          2047         361        1686
#

# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4 IPV6
aio1_cinder_api_container-ade654c3     RUNNING 1         onboot, openstack -    -
aio1_designate_container-1a360417      RUNNING 1         onboot, openstack -    -
aio1_galera_container-fd677ef1         RUNNING 1         onboot, openstack -    -
aio1_glance_container-03078ce1         RUNNING 1         onboot, openstack -    -
aio1_heat_api_container-db22c2e3       RUNNING 1         onboot, openstack -    -
aio1_horizon_container-cb145a57        RUNNING 1         onboot, openstack -    -
aio1_keystone_container-1753b49d       RUNNING 1         onboot, openstack -    -
aio1_memcached_container-4cb65778      RUNNING 1         onboot, openstack -    -
aio1_neutron_server_container-75989006 RUNNING 1         onboot, openstack -    -
aio1_nova_api_container-9045adf7       RUNNING 1         onboot, openstack -    -
aio1_rabbit_mq_container-1247be91      RUNNING 1         onboot, openstack -    -
aio1_repo_container-e846b452           RUNNING 1         onboot, openstack -    -
aio1_rsyslog_container-2845845e        RUNNING 1         onboot, openstack -    -
aio1_swift_proxy_container-6f69669c    RUNNING 1         onboot, openstack -    -
aio1_utility_container-8d3db5a0        RUNNING 1         onboot, openstack -    -
#

# bridge link
7: br-vlan-veth state UP @eth12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master br-vlan state forwarding priority 32 cost 2
11: vethIAS75F state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
13: vethN938XE state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
15: vethSLM8F1 state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
17: vethYFA2WQ state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
19: vethHA8ANX state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
21: vethDVORXC state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
23: vethJGJ1MS state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
25: veth36F943 state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
27: veth21PQ2R state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
29: vethG3QL94 state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
31: veth74OGXW state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
33: veth53KP4T state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
35: vethAPIEYQ state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
37: veth6BY9AS state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2
39: vethU02YU6 state UP @(null): <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master lxcbr0 state forwarding priority 32 cost 2

# brctl show
bridge name	bridge id		STP enabled	interfaces
br-mgmt		8000.000000000000	no
br-storage		8000.000000000000	no
br-vlan		8000.c22fbc05933f	no		br-vlan-veth
br-vxlan		8000.000000000000	no
lxcbr0		8000.fe030a91e2b0	no		veth21PQ2R
							veth36F943
							veth53KP4T
							veth6BY9AS
							veth74OGXW
							vethAPIEYQ
							vethDVORXC
							vethG3QL94
							vethHA8ANX
							vethIAS75F
							vethJGJ1MS
							vethN938XE
							vethSLM8F1
							vethU02YU6
							vethYFA2WQ
#

TASK [lxc_container_create : Add veth pair name to match container name]

TASK [lxc_container_create : Run container veth wiring script]





# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4           IPV6
aio1_cinder_api_container-ade654c3     RUNNING 1         onboot, openstack -              -
aio1_designate_container-1a360417      RUNNING 1         onboot, openstack -              -
aio1_galera_container-fd677ef1         RUNNING 1         onboot, openstack -              -
aio1_glance_container-03078ce1         RUNNING 1         onboot, openstack -              -
aio1_heat_api_container-db22c2e3       RUNNING 1         onboot, openstack -              -
aio1_horizon_container-cb145a57        RUNNING 1         onboot, openstack 172.29.236.87  -
aio1_keystone_container-1753b49d       RUNNING 1         onboot, openstack 172.29.237.126 -
aio1_memcached_container-4cb65778      RUNNING 1         onboot, openstack -              -
aio1_neutron_server_container-75989006 RUNNING 1         onboot, openstack -              -
aio1_nova_api_container-9045adf7       RUNNING 1         onboot, openstack -              -
aio1_rabbit_mq_container-1247be91      RUNNING 1         onboot, openstack -              -
aio1_repo_container-e846b452           RUNNING 1         onboot, openstack -              -
aio1_rsyslog_container-2845845e        RUNNING 1         onboot, openstack -              -
aio1_swift_proxy_container-6f69669c    RUNNING 1         onboot, openstack 172.29.238.119 -
aio1_utility_container-8d3db5a0        RUNNING 1         onboot, openstack 172.29.239.191 -
#


# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4                           IPV6
aio1_cinder_api_container-ade654c3     RUNNING 1         onboot, openstack 172.29.236.153, 172.29.244.182 -
aio1_designate_container-1a360417      RUNNING 1         onboot, openstack 172.29.238.217                 -
aio1_galera_container-fd677ef1         RUNNING 1         onboot, openstack 172.29.239.221                 -
aio1_glance_container-03078ce1         RUNNING 1         onboot, openstack 172.29.237.247, 172.29.245.112 -
aio1_heat_api_container-db22c2e3       RUNNING 1         onboot, openstack 172.29.238.75                  -
aio1_horizon_container-cb145a57        RUNNING 1         onboot, openstack 172.29.236.87                  -
aio1_keystone_container-1753b49d       RUNNING 1         onboot, openstack 172.29.237.126                 -
aio1_memcached_container-4cb65778      RUNNING 1         onboot, openstack 172.29.239.117                 -
aio1_neutron_server_container-75989006 RUNNING 1         onboot, openstack 172.29.238.16                  -
aio1_nova_api_container-9045adf7       RUNNING 1         onboot, openstack 172.29.237.23                  -
aio1_rabbit_mq_container-1247be91      RUNNING 1         onboot, openstack 172.29.238.162                 -
aio1_repo_container-e846b452           RUNNING 1         onboot, openstack 172.29.236.76                  -
aio1_rsyslog_container-2845845e        RUNNING 1         onboot, openstack 172.29.236.64                  -
aio1_swift_proxy_container-6f69669c    RUNNING 1         onboot, openstack 172.29.238.119, 172.29.245.33  -
aio1_utility_container-8d3db5a0        RUNNING 1         onboot, openstack 172.29.239.191                 -
#

# lxc-info --name aio1_horizon_container-cb145a57
Name:           aio1_horizon_container-cb145a57
State:          RUNNING
PID:            22361
IP:             172.29.236.87
CPU use:        6.92 seconds
BlkIO use:      713.13 MiB
Memory use:     1.96 MiB
KMem use:       0 bytes
Link:           vethN938XE
 TX bytes:      656 bytes
 RX bytes:      9.61 KiB
 Total bytes:   10.25 KiB
#


# top -n 1

top - 13:41:28 up  1:19,  2 users,  load average: 36.88, 16.26, 9.33
Tasks: 362 total,  41 running, 316 sleeping,   0 stopped,   5 zombie
%Cpu(s):  6.2 us, 46.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 47.7 si,  0.0 st
KiB Mem :   498888 total,     6360 free,   279208 used,   213320 buff/cache
KiB Swap:  2097148 total,  1752060 free,   345088 used.    93948 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
  33 root      20   0       0      0      0 R 12.7  0.0   0:36.13 kswapd0

TASK [lxc_container_create : Wait for container connectivity]

# ip -4 -o a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
2: eth0    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0\       valid_lft 81556sec preferred_lft 81556sec
3: br-mgmt    inet 172.29.236.100/22 brd 172.29.239.255 scope global br-mgmt\       valid_lft forever preferred_lft forever
4: br-storage    inet 172.29.244.100/22 brd 172.29.247.255 scope global br-storage\       valid_lft forever preferred_lft forever
5: br-vlan    inet 172.29.248.100/22 brd 172.29.251.255 scope global br-vlan\       valid_lft forever preferred_lft forever
5: br-vlan    inet 172.29.248.1/22 brd 172.29.251.255 scope global secondary br-vlan:0\       valid_lft forever preferred_lft forever
8: br-vxlan    inet 172.29.240.100/22 brd 172.29.243.255 scope global br-vxlan\       valid_lft forever preferred_lft forever
9: lxcbr0    inet 10.255.255.1/24 brd 10.255.255.255 scope global noprefixroute lxcbr0\       valid_lft forever preferred_lft forever
#

# brctl show
bridge name	bridge id		STP enabled	interfaces
br-mgmt		8000.fe188daeb570	no		03078ce1_eth1
							1247be91_eth1
							1753b49d_eth1
							1a360417_eth1
							2845845e_eth1
							4cb65778_eth1
							6f69669c_eth1
							75989006_eth1
							8d3db5a0_eth1
							9045adf7_eth1
							ade654c3_eth1
							cb145a57_eth1
							db22c2e3_eth1
							e846b452_eth1
							fd677ef1_eth1
br-storage		8000.fe17a5e00672	no		03078ce1_eth2
							6f69669c_eth2
							ade654c3_eth2
br-vlan		8000.c22fbc05933f	no		br-vlan-veth
br-vxlan		8000.000000000000	no
lxcbr0		8000.fe05dbf1432f	no		03078ce1_eth0
							1247be91_eth0
							1753b49d_eth0
							1a360417_eth0
							2845845e_eth0
							4cb65778_eth0
							6f69669c_eth0
							75989006_eth0
							8d3db5a0_eth0
							9045adf7_eth0
							ade654c3_eth0
							cb145a57_eth0
							db22c2e3_eth0
							e846b452_eth0
							fd677ef1_eth0
#


# lxc-ls --fancy
NAME                                   STATE   AUTOSTART GROUPS            IPV4           IPV6
aio1_cinder_api_container-ade654c3     RUNNING 1         onboot, openstack 10.255.255.233 -
aio1_designate_container-1a360417      RUNNING 1         onboot, openstack 10.255.255.230 -
aio1_galera_container-fd677ef1         RUNNING 1         onboot, openstack 10.255.255.204 -
aio1_glance_container-03078ce1         RUNNING 1         onboot, openstack 10.255.255.34  -
aio1_heat_api_container-db22c2e3       RUNNING 1         onboot, openstack 10.255.255.131 -
aio1_horizon_container-cb145a57        RUNNING 1         onboot, openstack 10.255.255.158 -
aio1_keystone_container-1753b49d       RUNNING 1         onboot, openstack 10.255.255.248 -
aio1_memcached_container-4cb65778      RUNNING 1         onboot, openstack 10.255.255.225 -
aio1_neutron_server_container-75989006 RUNNING 1         onboot, openstack 10.255.255.180 -
aio1_nova_api_container-9045adf7       RUNNING 1         onboot, openstack 10.255.255.43  -
aio1_rabbit_mq_container-1247be91      RUNNING 1         onboot, openstack 10.255.255.160 -
aio1_repo_container-e846b452           RUNNING 1         onboot, openstack 10.255.255.184 -
aio1_rsyslog_container-2845845e        RUNNING 1         onboot, openstack 10.255.255.133 -
aio1_swift_proxy_container-6f69669c    RUNNING 1         onboot, openstack 10.255.255.111 -
aio1_utility_container-8d3db5a0        RUNNING 1         onboot, openstack 10.255.255.93  -
#

# cat /etc/hosts
127.0.0.1 localhost aio1
127.0.1.1 aio1.openstack.local aio1

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
172.29.236.100 aio1.openstack.local aio1
172.29.239.191 aio1-utility-container-8d3db5a0.openstack.local aio1-utility-container-8d3db5a0 aio1_utility_container-8d3db5a0
172.29.238.119 aio1-swift-proxy-container-6f69669c.openstack.local aio1-swift-proxy-container-6f69669c aio1_swift_proxy_container-6f69669c
172.29.237.247 aio1-glance-container-03078ce1.openstack.local aio1-glance-container-03078ce1 aio1_glance_container-03078ce1
172.29.238.16 aio1-neutron-server-container-75989006.openstack.local aio1-neutron-server-container-75989006 aio1_neutron_server_container-75989006
172.29.236.76 aio1-repo-container-e846b452.openstack.local aio1-repo-container-e846b452 aio1_repo_container-e846b452
172.29.239.117 aio1-memcached-container-4cb65778.openstack.local aio1-memcached-container-4cb65778 aio1_memcached_container-4cb65778
172.29.239.221 aio1-galera-container-fd677ef1.openstack.local aio1-galera-container-fd677ef1 aio1_galera_container-fd677ef1
172.29.238.162 aio1-rabbit-mq-container-1247be91.openstack.local aio1-rabbit-mq-container-1247be91 aio1_rabbit_mq_container-1247be91
172.29.238.217 aio1-designate-container-1a360417.openstack.local aio1-designate-container-1a360417 aio1_designate_container-1a360417
172.29.236.153 aio1-cinder-api-container-ade654c3.openstack.local aio1-cinder-api-container-ade654c3 aio1_cinder_api_container-ade654c3
172.29.236.87 aio1-horizon-container-cb145a57.openstack.local aio1-horizon-container-cb145a57 aio1_horizon_container-cb145a57
172.29.237.126 aio1-keystone-container-1753b49d.openstack.local aio1-keystone-container-1753b49d aio1_keystone_container-1753b49d
172.29.238.75 aio1-heat-api-container-db22c2e3.openstack.local aio1-heat-api-container-db22c2e3 aio1_heat_api_container-db22c2e3
172.29.237.23 aio1-nova-api-container-9045adf7.openstack.local aio1-nova-api-container-9045adf7 aio1_nova_api_container-9045adf7
172.29.236.64 aio1-rsyslog-container-2845845e.openstack.local aio1-rsyslog-container-2845845e aio1_rsyslog_container-2845845e
#

TASK [openstack_hosts : If a keyfile is provided, copy the gpg keyfile to the key location]

# pstree
systemd─┬─NetworkManager─┬─dhclient
        │                └─2*[{NetworkManager}]
        ├─agetty
        ├─auditd───{auditd}
        ├─chronyd
        ├─crond
        ├─dbus-daemon───{dbus-daemon}
        ├─dnsmasq
        ├─gssproxy───5*[{gssproxy}]
        ├─lvmetad
        ├─15*[lxc-start───systemd─┬─5*[agetty]]
        │                         ├─crond]
        │                         ├─dbus-daemon]
        │                         ├─dhclient]
        │                         ├─rsyslogd───2*[{rsyslogd}]]
        │                         ├─sshd]
        │                         ├─systemd-journal]
        │                         └─systemd-logind]
        ├─master─┬─pickup
        │        └─qmgr
        ├─polkitd───6*[{polkitd}]
        ├─rpcbind
        ├─rsyslogd───5*[{rsyslogd}]
        ├─ssh
        ├─sshd───sshd───bash───sudo───bash───bash───ansible-playboo─┬─5*[ansible-playboo───ssh]
        │                                                           └─2*[{ansible-playboo}]
        ├─sshd───sshd───bash───sudo───bash───pstree
        ├─sshd───sshd───5*[lxc-attach───su───sh───python───python───yum]
        ├─systemd-journal
        ├─systemd-logind
        ├─systemd-udevd
        └─tuned───4*[{tuned}]
#

TASK [openstack_hosts : Add requirement packages (repositories gpg keys packages, toolkits...)]

# free -m
              total        used        free      shared  buff/cache   available
Mem:            487         371           6           0         109          45
Swap:          2047        1442         605
#


[ERROR]: User interrupted execution

# openstack-ansible setup-hosts.yml

so again?!
* part 2 on virtualbox with many interfaces

"vm_deployer_1563794608410_18016" 
"vm_control_1563795073812_19332"
"vm_compute_1563795546380_95137"
"vm_storage_1563796007834_66559"

vboxmanage modifyvm "vm_deployer_1563794608410_18016" --hostonlyadapter4 "VirtualBox Host-Only Ethernet Adapter #3"
vboxmanage modifyvm "vm_deployer_1563794608410_18016" --nic4 hostonly
vboxmanage modifyvm "vm_deployer_1563794608410_18016" --hostonlyadapter5 "VirtualBox Host-Only Ethernet Adapter #4"
vboxmanage modifyvm "vm_deployer_1563794608410_18016" --nic5 hostonly
vboxmanage modifyvm "vm_deployer_1563794608410_18016" --hostonlyadapter6 "VirtualBox Host-Only Ethernet Adapter #5"
vboxmanage modifyvm "vm_deployer_1563794608410_18016" --nic6 hostonly

vboxmanage modifyvm "vm_control_1563795073812_19332" --hostonlyadapter4 "VirtualBox Host-Only Ethernet Adapter #3"
vboxmanage modifyvm "vm_control_1563795073812_19332" --nic4 hostonly
vboxmanage modifyvm "vm_control_1563795073812_19332" --hostonlyadapter5 "VirtualBox Host-Only Ethernet Adapter #4"
vboxmanage modifyvm "vm_control_1563795073812_19332" --nic5 hostonly
vboxmanage modifyvm "vm_control_1563795073812_19332" --hostonlyadapter6 "VirtualBox Host-Only Ethernet Adapter #5"
vboxmanage modifyvm "vm_control_1563795073812_19332" --nic6 hostonly

vboxmanage modifyvm "vm_compute_1563795546380_95137" --hostonlyadapter4 "VirtualBox Host-Only Ethernet Adapter #3"
vboxmanage modifyvm "vm_compute_1563795546380_95137" --nic4 hostonly
vboxmanage modifyvm "vm_compute_1563795546380_95137" --hostonlyadapter5 "VirtualBox Host-Only Ethernet Adapter #4"
vboxmanage modifyvm "vm_compute_1563795546380_95137" --nic5 hostonly
vboxmanage modifyvm "vm_compute_1563795546380_95137" --hostonlyadapter6 "VirtualBox Host-Only Ethernet Adapter #5"
vboxmanage modifyvm "vm_compute_1563795546380_95137" --nic6 hostonly

vboxmanage modifyvm "vm_storage_1563796007834_66559" --hostonlyadapter4 "VirtualBox Host-Only Ethernet Adapter #3"
vboxmanage modifyvm "vm_storage_1563796007834_66559" --nic4 hostonly
vboxmanage modifyvm "vm_storage_1563796007834_66559" --hostonlyadapter5 "VirtualBox Host-Only Ethernet Adapter #4"
vboxmanage modifyvm "vm_storage_1563796007834_66559" --nic5 hostonly
vboxmanage modifyvm "vm_storage_1563796007834_66559" --hostonlyadapter6 "VirtualBox Host-Only Ethernet Adapter #5"
vboxmanage modifyvm "vm_storage_1563796007834_66559" --nic6 hostonly

vboxmanage showvminfo "vm_deployer_1563794608410_18016"  | more

yum install python-ethtool net-tools bridge-utils -y

* part 2 and fail to conn
  
** opendev.org connection fail

http://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt?id=6a92e89c14e68c42a149b719d93742979d241c5b
[2019-07-25 목 17:37] 오랜 실패의 시간을 소진하고 이제 된다. 이거참
아토에서 했으면 잘 됬을까?

열심히 그리고 opendev.org에서 파일을 받아온다.
- [ ] 오프라인으로 처리할 필요가 있다.

** bootstrap-aio.sh but syntax fail but right one as I can see

export PATH=/usr/local/bin:$PATH

then work! my

grep 50 tests/roles/bootstrap-host/defaults/main.yml
bootstrap_host_data_disk_min_size: 30

sed -i 's/50/30/' tests/roles/bootstrap-host/defaults/main.yml

* openstack-ansible setup-hosts.yml , hold again

TASK [ansible-hardening : Add or remove packages based on STIG requirements]
Friday 26 July 2019  11:43:58 +0900 (0:00:00.703)       0:06:00.246 
ok: [compute1] => (item=absent)
ok: [infra1] => (item=absent)
ok: [storage1] => (item=absent)

Ctrl-C

[root@control my]# rpm -qa
error: rpmdb: BDB0113 Thread/process 4426/140274796144704 failed: BDB1507 Thread died in Berkeley DB library
error: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery
error: cannot open Packages index using db5 -  (-30973)
error: cannot open Packages database in /var/lib/rpm
error: rpmdb: BDB0113 Thread/process 4426/140274796144704 failed: BDB1507 Thread died in Berkeley DB library
error: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery
error: cannot open Packages database in /var/lib/rpm
[root@control my]# 

rm -f /var/lib/rpm/__db*
rpm --rebuilddb

on control and storage, wait long have to wait long enough

/opt/openstack-ansible/playbooks/ 
setup-hosts.yml
setup-infrastructure.hml

/etc/openstack-deploy/ 
openstack_user_config.yml 
user_variables.yml

ansible-hardening

TASK [lxc_container_create : Create container (dir)] ***********************************************************************************************************
Friday 26 July 2019  13:43:30 +0900 (0:00:01.893)       0:12:04.867 *********** 
fatal: [infra1_keystone_container-af9f1617 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_neutron_server_container-1dbe0b7a -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_utility_container-0674895c -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_horizon_container-fbd3b559 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_repo_container-1de92927 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_glance_container-ed0fcd59 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_memcached_container-7d277f71 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_cinder_api_container-6d9d4294 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_heat_api_container-3ca87b11 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_galera_container-11734601 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_nova_api_container-f34593d8 -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}
fatal: [infra1_rabbit_mq_container-c34abfbe -> 172.29.236.11]: FAILED! => {"changed": false, "failed": true, "msg": "The `lxc` module is not importable. Check the requirements."}

PLAY RECAP *****************************************************************************************************************************************************
aio1                       : ok=0    changed=0    unreachable=1    failed=0   
aio1_cinder_api_container-c5058329 : ok=0    changed=0    unreachable=1    failed=0   
aio1_designate_container-0adb3958 : ok=0    changed=0    unreachable=1    failed=0   
aio1_galera_container-ffeb3190 : ok=0    changed=0    unreachable=1    failed=0   
aio1_glance_container-95d47c66 : ok=0    changed=0    unreachable=1    failed=0   
aio1_heat_api_container-0d531822 : ok=0    changed=0    unreachable=1    failed=0   
aio1_horizon_container-5d922e17 : ok=0    changed=0    unreachable=1    failed=0   
aio1_keystone_container-249da988 : ok=0    changed=0    unreachable=1    failed=0   
aio1_memcached_container-6405a10f : ok=0    changed=0    unreachable=1    failed=0   
aio1_neutron_server_container-5a9d0180 : ok=0    changed=0    unreachable=1    failed=0   
aio1_nova_api_container-a9c398e3 : ok=0    changed=0    unreachable=1    failed=0   
aio1_rabbit_mq_container-fcc2a0c8 : ok=0    changed=0    unreachable=1    failed=0   
aio1_repo_container-da809349 : ok=0    changed=0    unreachable=1    failed=0   
aio1_rsyslog_container-d3f65cc9 : ok=0    changed=0    unreachable=1    failed=0   
aio1_swift_proxy_container-ad4f8b12 : ok=0    changed=0    unreachable=1    failed=0   
aio1_utility_container-67d3edf4 : ok=0    changed=0    unreachable=1    failed=0   
compute1                   : ok=151  changed=25   unreachable=0    failed=0   
infra1                     : ok=54   changed=2    unreachable=0    failed=1   
infra1_cinder_api_container-6d9d4294 : ok=6    changed=2    unreachable=0    failed=1   
infra1_galera_container-11734601 : ok=6    changed=2    unreachable=0    failed=1   
infra1_glance_container-ed0fcd59 : ok=6    changed=2    unreachable=0    failed=1   
infra1_heat_api_container-3ca87b11 : ok=6    changed=2    unreachable=0    failed=1   
infra1_horizon_container-fbd3b559 : ok=6    changed=2    unreachable=0    failed=1   
infra1_keystone_container-af9f1617 : ok=6    changed=2    unreachable=0    failed=1   
infra1_memcached_container-7d277f71 : ok=6    changed=2    unreachable=0    failed=1   
infra1_neutron_server_container-1dbe0b7a : ok=6    changed=2    unreachable=0    failed=1   
infra1_nova_api_container-f34593d8 : ok=6    changed=2    unreachable=0    failed=1   
infra1_rabbit_mq_container-c34abfbe : ok=6    changed=2    unreachable=0    failed=1   
infra1_repo_container-1de92927 : ok=6    changed=2    unreachable=0    failed=1   
infra1_utility_container-0674895c : ok=6    changed=2    unreachable=0    failed=1   
storage1                   : ok=53   changed=2    unreachable=0    failed=1   

Friday 26 July 2019  13:43:36 +0900 (0:00:06.224)       0:12:11.093 *********** 
=============================================================================== 
ansible-hardening : Ensure RPM verification task has finished ----------------------------------------------------------------------------------------- 171.34s
lxc_container_create : Allow the usage of local facts ------------------------------------------------------------------------------------------------- 137.37s
Ensure python is installed ----------------------------------------------------------------------------------------------------------------------------- 44.40s
openstack_hosts : Drop hosts file entries script locally ----------------------------------------------------------------------------------------------- 28.73s
openstack_hosts : Adding new system tuning ------------------------------------------------------------------------------------------------------------- 20.41s
lxc_container_create : Container service directories --------------------------------------------------------------------------------------------------- 19.93s
ansible-hardening : Check each user to see if its home directory exists on the filesystem -------------------------------------------------------------- 17.86s
openstack_hosts : Load kernel module(s) ---------------------------------------------------------------------------------------------------------------- 17.38s
ansible-hardening : Add or remove packages based on STIG requirements ---------------------------------------------------------------------------------- 17.23s
pip_install : Install PIP ------------------------------------------------------------------------------------------------------------------------------ 15.56s
lxc_container_create : LXC autodev setup --------------------------------------------------------------------------------------------------------------- 11.08s
ansible-hardening : V-72269 - Synchronize system clock (configuration file) ----------------------------------------------------------------------------- 9.76s
lxc_container_create : Gather variables for each operating system --------------------------------------------------------------------------------------- 9.58s
lxc_container_create : Read custom facts from previous runs --------------------------------------------------------------------------------------------- 8.94s
ansible-hardening : Set sysctl configurations ----------------------------------------------------------------------------------------------------------- 7.16s
lxc_container_create : Create container (dir) ----------------------------------------------------------------------------------------------------------- 6.22s
openstack_hosts : Write list of modules to load at boot ------------------------------------------------------------------------------------------------- 4.88s
ansible-hardening : Adjust auditd/audispd configurations ------------------------------------------------------------------------------------------------ 3.75s
openstack_hosts : If a keyfile is provided, copy the gpg keyfile to the key location -------------------------------------------------------------------- 3.54s
ansible-hardening : Deploy rules for auditd based on STIG requirements ---------------------------------------------------------------------------------- 3.24s

EXIT NOTICE [Playbook execution failure] **************************************
===============================================================================


[root@storage ~]# rpm -qa
error: rpmdb: BDB0113 Thread/process 5878/139940613572672 failed: BDB1507 Thread died in Berkeley DB library
error: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery
error: cannot open Packages index using db5 -  (-30973)
error: cannot open Packages database in /var/lib/rpm
error: rpmdb: BDB0113 Thread/process 5878/139940613572672 failed: BDB1507 Thread died in Berkeley DB library
error: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery
error: cannot open Packages database in /var/lib/rpm
[root@storage ~]# rm -f /var/lib/rpm/__db.00*
[root@storage ~]# rpm --rebuilddb
[root@storage ~]# yum-complete-transaction 
Loaded plugins: fastestmirror, priorities, rpm-warm-cache
Loading mirror speeds from cached hostfile
 * base: mirror.navercorp.com
 * epel: mirror.horizon.vn
 * extras: mirror.navercorp.com
 * updates: mirror.navercorp.com
555 packages excluded due to repository priority protections
There are 1 outstanding transactions to complete. Finishing the most recent one
The remaining transaction had 5 elements left to run
Package aide-0.15.1-13.el7.x86_64 already installed and latest version
Package audispd-plugins-2.8.4-4.el7.x86_64 already installed and latest version
--> Running transaction check
---> Package dracut-fips.x86_64 0:033-554.el7 will be installed
---> Package dracut-fips-aesni.x86_64 0:033-554.el7 will be installed
---> Package hmaccalc.x86_64 0:0.9.13-4.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==============================================================================================================================================================================================================================================================================
 Package                                                                  Arch                                                          Version                                                             Repository                                                   Size
==============================================================================================================================================================================================================================================================================
Installing:
 dracut-fips                                                              x86_64                                                        033-554.el7                                                         base                                                         61 k
 dracut-fips-aesni                                                        x86_64                                                        033-554.el7                                                         base                                                         65 k
 hmaccalc                                                                 x86_64                                                        0.9.13-4.el7                                                        base                                                         26 k

Transaction Summary
==============================================================================================================================================================================================================================================================================
Install  3 Packages

Total size: 152 k
Installed size: 125 k
Is this ok [y/d/N]: y
Downloading packages:
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : hmaccalc-0.9.13-4.el7.x86_64                                                                                                                                                                                                                               1/3 
  Installing : dracut-fips-033-554.el7.x86_64                                                                                                                                                                                                                             2/3 
  Installing : dracut-fips-aesni-033-554.el7.x86_64                                                                                                                                                                                                                       3/3 
  Verifying  : hmaccalc-0.9.13-4.el7.x86_64                                                                                                                                                                                                                               1/3 
  Verifying  : dracut-fips-033-554.el7.x86_64                                                                                                                                                                                                                             2/3 
  Verifying  : dracut-fips-aesni-033-554.el7.x86_64                                                                                                                                                                                                                       3/3 

Installed:
  dracut-fips.x86_64 0:033-554.el7                                                        dracut-fips-aesni.x86_64 0:033-554.el7                                                        hmaccalc.x86_64 0:0.9.13-4.el7                                                       

Complete!
Cleaning up completed transaction file
[root@storage ~]#

rpmdb는 왜 이리도 깨지는 것일까? 이유는 무엇일까?

[root@deployer playbooks]# cat setup-hosts.yml 
---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- include: openstack-hosts-setup.yml
# - include: security-hardening.yml
- include: containers-deploy.yml
[root@deployer playbooks]# 

/etc/hosts 가 문제였을까?
aio으로 설치되었던 잔재가 문제가 되었을 소지가 있다. 

- [X] add more memory and cpu to compute
  - kswapd0 또 올라왔다. 

#+BEGIN_SRC 
top - 15:18:29 up  1:50,  1 user,  load average: 4.15, 3.03, 2.07
Tasks: 348 total,   6 running, 342 sleeping,   0 stopped,   0 zombie
%Cpu(s): 93.4 us,  5.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  1.2 si,  0.0 st
KiB Mem :  1881840 total,    82308 free,   745796 used,  1053736 buff/cache
KiB Swap:  1572860 total,  1464564 free,   108296 used.   845500 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                    
23559 root      20   0  414184 112496   9464 R  91.4  6.0   0:12.10 python   
#+END_SRC

- [X] care about facts at /etc/openstack_deploy/ansible_facts/
  - 25일 어제 밤 9시 34분에서 부터

다시 또 다시 그리고 또 다시다.

ls
ls -l
ls -al
ls -ltr
ls -altr
ls -lR
ls -lR | grep x


- [X] 개별서버의 /etc/hosts도 확인 필요하다, 제거 대상인가?
- [ ] lxc howto

* then again

- [X] openstack-ansible setup-infrastructure.yml --syntax-check
- [ ] openstack-ansible setup-hosts.yml

infra1 is the 'control' but who name it?
compute1
storage1

ASK [Ensure python is installed] ******************************************************************************************************************************
Friday 26 July 2019  15:30:34 +0900 (0:00:01.871)       0:00:01.871 *********** 
ok: [infra1]
ok: [compute1]
ok: [storage1]
fatal: [aio1]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: ssh: connect to host 172.29.236.100 port 22: No route to host\r\n", "unreachable": true}


/tmp/

TASK [Ensure python is installed] ******************************************************************************************************************************
Friday 26 July 2019  15:36:09 +0900 (0:00:01.239)       0:00:01.239 *********** 
ok: [compute1]
ok: [infra1]
ok: [storage1]
fatal: [aio1]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: 
ssh: connect to host 172.29.236.100 port 22: No route to host\r\n", "unreachable": true}



- name: Install Ansible prerequisites
  hosts: "{{ openstack_host_group|default('hosts') }}"
  gather_facts: false
  user: root
  pre_tasks:
    - name: Ensure python is installed
      register: result
      raw: |
        if which apt-get >/dev/null && ! which python >/dev/null ; then
          apt-get -y install python
          exit 2
        else
          exit 0
        fi
      changed_when: "result.rc == 2"
      failed_when: "result.rc not in [0, 2]"



https://stackoverflow.com/questions/42971296/usage-of-variable-and-role-in-openstack-ansible

* then again, 다시

- [X] script/bootstrap-ansible.sh
- [X] openstack-ansible setup-infrastructure.yml --syntax-check
- [ ] openstack-ansible setup-hosts.yml

이제는 Ensure python is installed에서 aio1에 대한 실패는 뜨지 않겠지? 인데
서 있다. 설마!

또 떴다. aio1 그리고 172.29.236.100

rpm 또 깨질까? 그러면 그때 comment

# Bind the External VIP
auto br-mgmt:0
iface br-mgmt:0 inet static
    address 172.29.236.10
    netmask 255.255.252.0

global_overrides:
  # The internal and external VIP should be different IPs, however they
  # do not need to be on separate networks.
  external_lb_vip_address: 172.29.236.10

https://docs.openstack.org/openstack-ansible/queens/user/test/example.html


[root@deployer openstack_deploy]# cat /etc/sysconfig/network-scripts/ifcfg-br-mgmt 
BOOTPROTO=none
ONBOOT=yes
DEVICE=br-mgmt
TYPE=Bridge
IPADDR=172.29.236.3
PREFIX=22
[root@deployer openstack_deploy]# 

https://ma.ttias.be/how-to-add-secondary-ip-alias-on-network-interface-in-rhel-centos-7/

# cat /etc/sysconfig/network-scripts/ifcfg-br-mgmt:0
BOOTPROTO=none
ONBOOT=yes
DEVICE=br-mgmt:0
TYPE=Bridge
IPADDR=172.29.236.10
PREFIX=22

그런데 어디에 두어야 하는가?

TASK [Ensure python is installed] ********************************************************************************************************************************************************************************************************************************************
task path: /opt/openstack-ansible/playbooks/openstack-hosts-setup.yml:27
Friday 26 July 2019  16:47:53 +0900 (0:00:01.302)       0:00:01.302 *********** 
<aio1> The "physical_host" variable of "aio1" has been found to have a corresponding host entry in inventory.
<aio1> The "physical_host" variable of "aio1" terminates at "172.29.236.100" using the host variable "ansible_host".
container_name: "aio1"
physical_host: "aio1"
container_name: "aio1"
physical_host: "aio1"
<172.29.236.100> ESTABLISH SSH CONNECTION FOR USER: root
<172.29.236.100> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=root -o ConnectTimeout=5 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ServerAliveInterval=64 -o ServerAliveCountMax=1024 -o Compression=no -o TCPKeepAlive=yes -o VerifyHostKeyDNS=no -o ForwardX11=no -o ForwardAgent=yes -T -o ControlPath=/root/.ansible/cp/53b4b74265 -tt 172.29.236.100 'if which apt-get >/dev/null && ! which python >/dev/null ; then
 apt-get -y install python
 exit 2
 else
 exit 0
 fi'


https://docs.openstack.org/openstack-ansible/pike/reference/manage-inventory.html

[root@deployer openstack_deploy]# cat openstack_inventory.json.old | grep 100
                "ansible_host": "172.29.236.100",
		

https://docs.openstack.org/openstack-ansible/latest/reference/inventory/manage-inventory.html

Never edit or delete the files /etc/openstack_deploy/openstack_inventory.json or /etc/openstack_deploy/openstack_hostnames_ips.yml. This can lead to file corruptions, and problems with the inventory: hosts and container could disappear and new ones would appear, breaking your existing deployment.

/etc/openstack-x
/opt/openstack-x

cleaning then again

* again from the beginning without reinstall os

nop more control compute storage related task except rpm

- [X] using pre.sh
- [X] openstack-ansible setup-infrastructure.yml --syntax-check

rpm broken

rm -f /var/lib/rpm/__db.00*
rpm --rebuilddb

nop hardening

- [ ] openstack-ansible setup-hosts.yml

nop disk space 50 to 30

이런 아래 때문일까?

TASK [lxc_container_create : Run container veth wiring script] *************************************************************************************************
Friday 26 July 2019  17:33:19 +0900 (0:00:10.897)       0:15:11.674 *********** 
 [WARNING]: sftp transfer mechanism failed on [172.29.236.11]. Use ANSIBLE_DEBUG=1 to see detailed information

 [WARNING]: scp transfer mechanism failed on [172.29.236.11]. Use ANSIBLE_DEBUG=1 to see detailed information

failed: [infra1_keystone_container-3c65c331] (item={'value': {u'interface': u'eth1', u'bridge': u'br-mgmt', u'netmask': u'255.255.252.0', u'type': u'veth', u'address': u'172.29.236.60'}, 'key': u'container_address'}) => {"item": {"key": "container_address", "value": {"address": "172.29.236.60", "bridge": "br-mgmt", "interface": "eth1", "netmask": "255.255.252.0", "type": "veth"}}, "msg": "Failed to connect to the host via ssh: ssh: connect to host 172.29.236.11 port 22: Connection timed out\r\n", "unreachable": true}
fatal: [infra1_keystone_container-3c65c331]: UNREACHABLE! => {"changed": false, "msg": "All items completed", "results": [{"_ansible_ignore_errors": null, "_ansible_item_result": true, "item": {"key": "container_address", "value": {"address": "172.29.236.60", "bridge": "br-mgmt", "interface": "eth1", "netmask": "255.255.252.0", "type": "veth"}}, "msg": "Failed to connect to the host via ssh: ssh: co

[root@deployer openstack_deploy]# cp user_variables.yml.test.example user_variables.yml
cp: overwrite ‘user_variables.yml’? y
[root@deployer openstack_deploy]# vi user_variables.yml
[root@deployer openstack_deploy]# 

ANSIBLE_DEBUG=1

# openstack-ansible setup-infrastructure.yml --syntax-check
Variable files: "-e @/etc/openstack_deploy/user_secrets.yml -e @/etc/openstack_deploy/user_variables.yml "
 [WARNING]: Unable to parse /etc/openstack_deploy/inventory.ini as an inventory source



 [WARNING]: Could not match supplied host pattern, ignoring: unbound
 [WARNING]: Could not match supplied host pattern, ignoring: repo_masters
 [WARNING]: Could not match supplied host pattern, ignoring: etcd_all
 [WARNING]: Could not match supplied host pattern, ignoring: ceph-mon
 [WARNING]: Could not match supplied host pattern, ignoring: ceph-osd
 [WARNING]: Could not match supplied host pattern, ignoring: rsyslog



TASK [Ensure python is installed] ******************************************************************
Friday 26 July 2019  21:22:25 +0900 (0:00:00.703)       0:00:00.703 *********** 
fatal: [compute1]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: ssh: connect to host 172.29.236.12 port 22: No route to host\r\n", "unreachable": true}
fatal: [infra1]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: ssh: connect to host 172.29.236.11 port 22: No route to host\r\n", "unreachable": true}
fatal: [storage1]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: ssh: connect to host 172.29.236.13 port 22: No route to host\r\n", "unreachable": true}

PLAY RECAP *****************************************************************************************
compute1                   : ok=0    changed=0    unreachable=1    failed=0   
infra1                     : ok=0    changed=0    unreachable=1    failed=0   
storage1                   : ok=0    changed=0    unreachable=1    failed=0   

Friday 26 July 2019  21:23:09 +0900 (0:00:44.547)       0:00:45.251 *********** 
=============================================================================== 
Ensure python is installed ----------------------------------------------------------------- 44.55s

EXIT NOTICE [Playbook execution failure] **************************************
===============================================================================


따라갈 수 없다. 눈 튀어나오겠다.
오늘 몇시부터 몇시까지 봤는지 모르겠다.
어쨌거나 녹색이 많이 나온다. 하얀색 그리고 녹색 좋은 로그다.

하얀색은? description and timestamp
TASK [lxc_hosts : Add image cache] ****************
Friday 26 July 2019  21:33:16 +0900 (0:00:00.925) 0:05:04:803

녹색은? 
ok: [infra1] => (item=logrotate)

노란색은? 
changed: [infra1]

푸른색은? 
include: /etc/ansible/roles/lxc_hosts/tasks/lxc_cache_preparation>systemd_old.yml for infra1
skipping: [infra1_rabbit_mq_container_x]

보라색은?

짙은 보라색은?

흑색은? 
FAILED - RETRYING: Ensure image has been pre-staged (60 retries left).

WARNING! The remote SSH server rejected X11 forwarding request.
Last login: Fri Jul 26 21:36:13 2019 from 192.168.100.1
------------------------------------------------------------------------------
* WARNING                                                                    *
* You are accessing a secured system and your actions will be logged along   *
* with identifying information. Disconnect immediately if you are not an     *
* authorized user of this system.                                            *
------------------------------------------------------------------------------

# cat /etc/ssh/sshd_config  | grep ^Banner
Banner /etc/motd

infra aka control 한참 설치 중에 들어가서 보니 
메모리 8g 중 4g 사용 중이다.
cpus는 20% 이하로 나온다. 좋다.

Friday 26 July 2019  21:59:41 +0900 (0:00:01.597)       0:31:30.193 *********** 
=============================================================================== 
pip_install : Install distro packages --------------------------------------------------------------------------- 234.25s
lxc_hosts : Ensure image has been pre-staged -------------------------------------------------------------------- 203.83s
openstack_hosts : Add requirement packages (repositories gpg keys packages, toolkits...) ------------------------ 196.86s
pip_install : Install PIP --------------------------------------------------------------------------------------- 106.75s
lxc_hosts : Ensure that the LXC cache has been prepared ---------------------------------------------------------- 96.81s
lxc_hosts : Place container rootfs ------------------------------------------------------------------------------- 63.33s
lxc_container_create : Write default container config ------------------------------------------------------------ 50.56s
pip_install : Get Modern PIP ------------------------------------------------------------------------------------- 48.18s
lxc_hosts : Ensure createrepo package is installed --------------------------------------------------------------- 41.32s
lxc_container_create : Container service directories ------------------------------------------------------------- 24.39s
lxc_container_create : Run container veth wiring script ---------------------------------------------------------- 22.16s
lxc_hosts : Create lxc image ------------------------------------------------------------------------------------- 21.51s
openstack_hosts : Adding new system tuning ----------------------------------------------------------------------- 21.35s
openstack_hosts : Load kernel module(s) -------------------------------------------------------------------------- 18.73s
lxc_container_create : Read custom facts from previous runs ------------------------------------------------------ 14.84s
pip_install : Install PIP ---------------------------------------------------------------------------------------- 14.46s
lxc_container_create : Drop veth cleanup script ------------------------------------------------------------------ 13.45s
openstack_hosts : If a keyfile is provided, copy the gpg keyfile to the key location ----------------------------- 13.26s
openstack_hosts : Drop hosts file entries script locally --------------------------------------------------------- 12.83s
lxc_container_create : LXC host config for container networks ---------------------------------------------------- 12.59s

EXIT NOTICE [Playbook execution success] **************************************
===============================================================================
? openstack-ansible setup-hosts.yml 

된것가 여기까지는 말이다.

? openstack-ansible setup-infrastructure.yml

이다. 

infra aka control 한참 설치 중에 들어가서 보니 
메모리 8g 중 4g free다. > 600m free다.
cpus는 20% 이하로 나온다. 좋다. > 최고 100%도 친다. 그래도 낮게 나온다. 

시간은 오래 걸린다 언제 끝이 날까?

1:16:15:242



ASK [galera_client : Install galera distro packages] ********************************************************************
Friday 26 July 2019  23:18:07 +0900 (0:00:00.659)       1:16:15.242 *********** 

TASK [galera_server : Fail if the galera root password is not provided] **************************************************
Friday 26 July 2019  23:32:25 +0900 (0:00:00.023)       1:30:33.507 *********** 
fatal: [infra1_galera_container-5103f3b8]: FAILED! => {"changed": false, "failed": true, "msg": "Please set the galera_root_password variable prior to applying the\ngalera role.\n"}

PLAY RECAP ***************************************************************************************************************
compute1                   : ok=18   changed=3    unreachable=0    failed=0   
infra1                     : ok=52   changed=25   unreachable=0    failed=0   
infra1_cinder_api_container-179eba5a : ok=9    changed=3    unreachable=0    failed=0   
infra1_galera_container-5103f3b8 : ok=20   changed=8    unreachable=0    failed=1   
infra1_glance_container-9e87261d : ok=9    changed=3    unreachable=0    failed=0   
infra1_heat_api_container-1e3d12d9 : ok=9    changed=3    unreachable=0    failed=0   
infra1_horizon_container-280f6f5a : ok=9    changed=3    unreachable=0    failed=0   
infra1_keystone_container-3c65c331 : ok=9    changed=3    unreachable=0    failed=0   
infra1_memcached_container-21e6d45f : ok=40   changed=22   unreachable=0    failed=0   
infra1_neutron_server_container-47a7eec0 : ok=9    changed=3    unreachable=0    failed=0   
infra1_nova_api_container-758f0659 : ok=9    changed=3    unreachable=0    failed=0   
infra1_rabbit_mq_container-76ec7893 : ok=9    changed=3    unreachable=0    failed=0   
infra1_repo_container-a2b512e9 : ok=176  changed=78   unreachable=0    failed=0   
infra1_utility_container-8de920e2 : ok=36   changed=20   unreachable=0    failed=0   
localhost                  : ok=1    changed=1    unreachable=0    failed=0   
storage1                   : ok=9    changed=3    unreachable=0    failed=0   

Friday 26 July 2019  23:32:25 +0900 (0:00:00.082)       1:30:33.590 *********** 
=============================================================================== 
repo_build : Create OpenStack-Ansible requirement wheels ------------------------------------------------------- 2445.79s
repo_build : Wait for the venvs builds to complete -------------------------------------------------------------- 792.19s
galera_client : Install galera distro packages ------------------------------------------------------------------ 661.04s
repo_build : Clone git repositories ----------------------------------------------------------------------------- 523.33s
galera_client : Install galera distro packages ------------------------------------------------------------------ 229.32s
memcached_server : Install distro packages ----------------------------------------------------------------------- 82.13s
repo_build : Install packages ------------------------------------------------------------------------------------ 53.20s
Install pip packages --------------------------------------------------------------------------------------------- 50.43s
repo_server : Install pip packages (from repo) ------------------------------------------------------------------- 35.12s
repo_server : Install distro packages ---------------------------------------------------------------------------- 34.20s
repo_build : Install pip packages (from repo) -------------------------------------------------------------------- 32.20s
repo_build : Execute the venv build scripts asynchonously -------------------------------------------------------- 29.20s
haproxy_server : Create haproxy service config files ------------------------------------------------------------- 28.73s
haproxy_server : Install HAProxy Packages ------------------------------------------------------------------------ 24.08s
repo_server : Install required pip packages (from repo) ---------------------------------------------------------- 19.98s
repo_build : Create venv build options files --------------------------------------------------------------------- 14.96s
pip_install : Install PIP ---------------------------------------------------------------------------------------- 10.98s
pip_install : Install PIP ---------------------------------------------------------------------------------------- 10.96s
Install distro packages ------------------------------------------------------------------------------------------- 8.58s
repo_server : File and directory setup (non-root user) ------------------------------------------------------------ 7.79s

EXIT NOTICE [Playbook execution failure] **************************************
===============================================================================
? 



- name: Fail if the galera root password is not provided
  fail:
    msg: |
      Please set the galera_root_password variable prior to applying the
      galera role.
  when: (galera_root_password is undefined) or (galera_root_password is none)
  tags:
    - always



grep -r galera_root_password *

? ./scripts/pw-token-gen.py --file /etc/openstack_deploy/user_secrets.yml
Creating backup file [ /etc/openstack_deploy/user_secrets.yml.tar ]
Operation Complete, [ /etc/openstack_deploy/user_secrets.yml ] is ready
? pwd
/opt/openstack-ansible
? 


? diff user_secrets.yml.orig user_secrets.yml | grep galera_root_password
< galera_root_password:
> galera_root_password: dcf6eb7100fb1f42ecb2
? 

* galera then again

[2019-07-26 금 23:41] 오늘은 이만하고 잘까? 이건 어쩌나? 두면 될까? 얼마나 걸리려나?
* done

PLAY [Ensure rabbitmq user for monitoring GUI] ***************************************************************************

TASK [Create rabbitmq user for monitoring GUI] ***************************************************************************
Saturday 27 July 2019  00:21:01 +0900 (0:00:00.438)       0:40:02.521 ********* 
changed: [infra1_rabbit_mq_container-76ec7893]
 [WARNING]: Could not match supplied host pattern, ignoring: etcd_all


PLAY [Install etcd server cluster] ***************************************************************************************
skipping: no hosts matched
 [WARNING]: Could not match supplied host pattern, ignoring: ceph-mon


PLAY [Install ceph mons] *************************************************************************************************
skipping: no hosts matched
 [WARNING]: Could not match supplied host pattern, ignoring: ceph-osd


PLAY [Install ceph osds] *************************************************************************************************
skipping: no hosts matched
 [WARNING]: Could not match supplied host pattern, ignoring: rsyslog


PLAY [Install rsyslog] ***************************************************************************************************
skipping: no hosts matched

PLAY RECAP ***************************************************************************************************************
compute1                   : ok=18   changed=0    unreachable=0    failed=0   
infra1                     : ok=48   changed=0    unreachable=0    failed=0   
infra1_cinder_api_container-179eba5a : ok=9    changed=0    unreachable=0    failed=0   
infra1_galera_container-5103f3b8 : ok=83   changed=40   unreachable=0    failed=0   
infra1_glance_container-9e87261d : ok=9    changed=0    unreachable=0    failed=0   
infra1_heat_api_container-1e3d12d9 : ok=9    changed=0    unreachable=0    failed=0   
infra1_horizon_container-280f6f5a : ok=9    changed=0    unreachable=0    failed=0   
infra1_keystone_container-3c65c331 : ok=9    changed=0    unreachable=0    failed=0   
infra1_memcached_container-21e6d45f : ok=34   changed=1    unreachable=0    failed=0   
infra1_neutron_server_container-47a7eec0 : ok=9    changed=0    unreachable=0    failed=0   
infra1_nova_api_container-758f0659 : ok=9    changed=0    unreachable=0    failed=0   
infra1_rabbit_mq_container-76ec7893 : ok=84   changed=41   unreachable=0    failed=0   
infra1_repo_container-a2b512e9 : ok=143  changed=8    unreachable=0    failed=0   
infra1_utility_container-8de920e2 : ok=32   changed=5    unreachable=0    failed=0   
localhost                  : ok=1    changed=1    unreachable=0    failed=0   
storage1                   : ok=9    changed=0    unreachable=0    failed=0   

Saturday 27 July 2019  00:21:16 +0900 (0:00:14.346)       0:40:16.867 ********* 
=============================================================================== 
galera_server : Install galera_server role remote packages ----------------------------------------------------- 1627.36s
rabbitmq_server : Ensure RabbitMQ node [0] is stopped ------------------------------------------------------------ 95.15s
rabbitmq_server : Install RabbitMQ packages ---------------------------------------------------------------------- 41.08s
haproxy_server : Create haproxy service config files ------------------------------------------------------------- 27.78s
rabbitmq_server : Configure rabbitmq plugins --------------------------------------------------------------------- 22.86s
rabbitmq_server : Ensure RabbitMQ node [0] is started ------------------------------------------------------------ 22.44s
rabbitmq_server : Ensure RabbitMQ node [0] is started ------------------------------------------------------------ 19.84s
rabbitmq_server : Ensure RabbitMQ node [0] is started ------------------------------------------------------------ 17.11s
rabbitmq_server : Install yum versionlock plugin ----------------------------------------------------------------- 16.97s
rabbitmq_server : Ensure default rabbitmq guest user is removed -------------------------------------------------- 14.47s
Create rabbitmq user for monitoring GUI -------------------------------------------------------------------------- 14.35s
galera_server : Run galera secure -------------------------------------------------------------------------------- 13.84s
haproxy_server : Install HAProxy Packages ------------------------------------------------------------------------ 12.46s
galera_server : Apply systemd options ---------------------------------------------------------------------------- 11.51s
rabbitmq_server : Ensure RabbitMQ node [0] is stopped ------------------------------------------------------------ 11.24s
pip_install : Install PIP ---------------------------------------------------------------------------------------- 10.99s
repo_build : Create venv build options files --------------------------------------------------------------------- 10.70s
galera_server : Start new cluster -------------------------------------------------------------------------------- 10.55s
haproxy_server : Remove haproxy service config files for absent services ------------------------------------------ 9.60s
repo_build : Install packages ------------------------------------------------------------------------------------- 7.54s

EXIT NOTICE [Playbook execution success] **************************************
===============================================================================
You have mail in /var/spool/mail/root
? timed out waiting for input: auto-logout
[vagrant@deployer ~]$ 


앗 아직이다.




[root@deployer playbooks]# openstack-ansible setup-openstack.yml 


TASK [os_nova : Install required pip packages] ***************************************************************************
Saturday 27 July 2019  01:06:11 +0900 (0:00:00.032)       0:24:45.984 ********* 

TASK [os_nova : Set SELinux file contexts for nova's ssh keys] ***********************************************************
Saturday 27 July 2019  01:07:18 +0900 (0:00:00.087)       0:25:53.501 ********* 
fatal: [compute1]: FAILED! => {"changed": false, "failed": true, "msg": "This module requires policycoreutils-python"}

RUNNING HANDLER [os_nova : Stop services] ********************************************************************************
Saturday 27 July 2019  01:07:19 +0900 (0:00:00.610)       0:25:54.111 ********* 
FAILED - RETRYING: Stop services (5 retries left).
FAILED - RETRYING: Stop services (4 retries left).
FAILED - RETRYING: Stop services (3 retries left).
FAILED - RETRYING: Stop services (2 retries left).
FAILED - RETRYING: Stop services (1 retries left).
failed: [compute1] (item={u'init_config_overrides': {}, u'service_name': u'nova-compute', u'start_order': 5, u'group': u'nova_compute', 'service_key': u'nova-compute'}) => {"attempts": 5, "changed": false, "failed": true, "item": {"group": "nova_compute", "init_config_overrides": {}, "service_key": "nova-compute", "service_name": "nova-compute", "start_order": 5}, "msg": "Could not find the requested service nova-compute: host"}

RUNNING HANDLER [os_nova : Copy new policy file into place] **************************************************************
Saturday 27 July 2019  01:07:32 +0900 (0:00:12.694)       0:26:06.806 ********* 

RUNNING HANDLER [os_nova : Remove legacy policy.json file] ***************************************************************
Saturday 27 July 2019  01:07:32 +0900 (0:00:00.034)       0:26:06.841 ********* 
ok: [compute1]

RUNNING HANDLER [os_nova : Start services] *******************************************************************************
Saturday 27 July 2019  01:07:32 +0900 (0:00:00.369)       0:26:07.211 ********* 
FAILED - RETRYING: Start services (5 retries left).
FAILED - RETRYING: Start services (4 retries left).
FAILED - RETRYING: Start services (3 retries left).
FAILED - RETRYING: Start services (2 retries left).
FAILED - RETRYING: Start services (1 retries left).
failed: [compute1] (item={u'init_config_overrides': {}, u'service_name': u'nova-compute', u'start_order': 5, u'group': u'nova_compute', 'service_key': u'nova-compute'}) => {"attempts": 5, "changed": false, "failed": true, "item": {"group": "nova_compute", "init_config_overrides": {}, "service_key": "nova-compute", "service_name": "nova-compute", "start_order": 5}, "msg": "Could not find the requested service nova-compute: host"}

RUNNING HANDLER [os_nova : Wait for the nova-compute service to initialize] **********************************************
Saturday 27 July 2019  01:07:45 +0900 (0:00:12.645)       0:26:19.857 ********* 
skipping: [compute1]

RUNNING HANDLER [os_nova : meta] *****************************************************************************************
Saturday 27 July 2019  01:07:45 +0900 (0:00:00.133)       0:26:19.990 ********* 

PLAY RECAP ***************************************************************************************************************
compute1                   : ok=37   changed=21   unreachable=0    failed=3   
infra1_cinder_api_container-179eba5a : ok=107  changed=58   unreachable=0    failed=0   
infra1_glance_container-9e87261d : ok=86   changed=52   unreachable=0    failed=0   
infra1_keystone_container-3c65c331 : ok=123  changed=66   unreachable=0    failed=0   
infra1_nova_api_container-758f0659 : ok=98   changed=60   unreachable=0    failed=0   
storage1                   : ok=72   changed=43   unreachable=0    failed=0   

Saturday 27 July 2019  01:07:45 +0900 (0:00:00.022)       0:26:20.013 ********* 
=============================================================================== 
os_nova : Install distro packages ------------------------------------------------------------------------------- 188.37s
Perform online data migrations ----------------------------------------------------------------------------------- 66.99s
os_cinder : Install distro packages ------------------------------------------------------------------------------ 65.97s
os_keystone : Install distro packages ---------------------------------------------------------------------------- 56.95s
os_nova : Install required pip packages -------------------------------------------------------------------------- 37.00s
os_nova : Synchronize the nova DB schema ------------------------------------------------------------------------- 33.08s
os_cinder : Ensure cinder api is available ----------------------------------------------------------------------- 32.60s
os_cinder : Install distro packages ------------------------------------------------------------------------------ 30.99s
os_glance : Install distro packages ------------------------------------------------------------------------------ 29.85s
os_nova : Install distro packages -------------------------------------------------------------------------------- 28.45s
os_nova : Install required pip packages -------------------------------------------------------------------------- 27.53s
os_keystone : Wait for web server to complete starting ----------------------------------------------------------- 21.32s
os_keystone : Wait for uWSGI socket to be ready ------------------------------------------------------------------ 21.22s
Ensure rabbitmq user --------------------------------------------------------------------------------------------- 20.34s
Ensure rabbitmq user --------------------------------------------------------------------------------------------- 17.36s
Ensure rabbitmq user --------------------------------------------------------------------------------------------- 16.69s
Ensure rabbitmq user --------------------------------------------------------------------------------------------- 15.63s
os_cinder : Install requires pip packages ------------------------------------------------------------------------ 15.53s
Ensure Rabbitmq vhost -------------------------------------------------------------------------------------------- 14.13s
os_nova : Unarchive pre-built venv ------------------------------------------------------------------------------- 13.93s

EXIT NOTICE [Playbook execution failure] **************************************
===============================================================================
[root@deployer playbooks]# 

[root@compute ~]# yum install policycoreutils-python -y

어 이거 그분이 올려놓은 글에서 확인한 내용인데

오래 걸린다. 이유는

control 8g 중 150m free, cpu hot with python, swap 427m using
compute 2g 중 822m free . 504m free.
storage 2g 중 645m free

[vagrant@compute ~]$ pstree -p
systemd(1)─┬─VBoxService(833)─┬─{VBoxService}(835)
           │                  ├─{VBoxService}(836)
           │                  ├─{VBoxService}(837)
           │                  ├─{VBoxService}(838)
           │                  ├─{VBoxService}(839)
           │                  ├─{VBoxService}(840)
           │                  └─{VBoxService}(841)
           ├─agetty(736)
           ├─auditd(646)───{auditd}(647)
           ├─chronyd(689)
           ├─crond(729)
           ├─dbus-daemon(680)───{dbus-daemon}(705)
           ├─dhclient(1049)
           ├─gssproxy(696)─┬─{gssproxy}(697)
           │               ├─{gssproxy}(698)
           │               ├─{gssproxy}(699)
           │               ├─{gssproxy}(700)
           │               └─{gssproxy}(701)
           ├─irqbalance(711)
           ├─libvirtd(8117)─┬─{libvirtd}(8118)
           │                ├─{libvirtd}(8119)
           │                ├─{libvirtd}(8120)
           │                ├─{libvirtd}(8121)
           │                ├─{libvirtd}(8122)
           │                ├─{libvirtd}(8123)
           │                ├─{libvirtd}(8124)
           │                ├─{libvirtd}(8125)
           │                ├─{libvirtd}(8126)
           │                ├─{libvirtd}(8127)
           │                ├─{libvirtd}(8128)
           │                ├─{libvirtd}(8129)
           │                ├─{libvirtd}(8130)
           │                ├─{libvirtd}(8131)
           │                ├─{libvirtd}(8132)
           │                └─{libvirtd}(8137)
           ├─lvmetad(492)


[vagrant@storage ~]$ pstree -p
systemd(1)─┬─VBoxService(834)─┬─{VBoxService}(836)
           │                  ├─{VBoxService}(837)
           │                  ├─{VBoxService}(838)
           │                  ├─{VBoxService}(839)
           │                  ├─{VBoxService}(840)
           │                  ├─{VBoxService}(841)
           │                  └─{VBoxService}(842)
           ├─agetty(728)
           ├─auditd(649)───{auditd}(650)
           ├─chronyd(696)
           ├─cinder-volume(6312)───cinder-volume(6323)


[vagrant@control ~]$ pstree
systemd─┬─VBoxService───7*[{VBoxService}]
        ├─agetty
        ├─auditd───{auditd}
        ├─chronyd
        ├─crond
        ├─dbus-daemon───{dbus-daemon}
        ├─dhclient
        ├─dnsmasq
        ├─gssproxy───5*[{gssproxy}]
        ├─haproxy-systemd───haproxy───haproxy
        ├─irqbalance
        ├─lvmetad
        ├─14*[lxc-autostart───systemd─┬─5*[agetty]]
        │                             ├─crond]
        │                             ├─dbus-daemon]
        │                             ├─dhclient]
        │                             ├─rsyslogd───2*[{rsyslogd}]]
        │                             ├─sshd]
        │                             ├─systemd-journal]
        │                             └─systemd-logind]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─anacron
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     ├─systemd-logind
        │                     └─uwsgi───8*[uwsgi]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─cinder-schedule
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     ├─systemd-logind
        │                     └─uwsgi───9*[uwsgi]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─beam.smp─┬─erl_child_setup───inet_gethost───inet_gethost
        │                     │          └─135*[{beam.smp}]
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─epmd
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     └─systemd-logind
        ├─2*[lxc-start───systemd─┬─5*[agetty]]
        │                        ├─crond]
        │                        ├─dbus-daemon]
        │                        ├─dhclient]
        │                        ├─rsyslogd───2*[{rsyslogd}]]
        │                        ├─sshd]
        │                        ├─systemd-journal]
        │                        └─systemd-logind]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─nova-conductor───2*[nova-conductor]
        │                     ├─nova-consoleaut
        │                     ├─nova-scheduler
        │                     ├─nova-spicehtml5
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     ├─systemd-logind
        │                     └─3*[uwsgi───9*[uwsgi]]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─anacron
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─nginx───4*[nginx]
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     ├─systemd-logind
        │                     └─2*[uwsgi───9*[uwsgi]]
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─memcached───9*[{memcached}]
        │                     ├─rsyslogd───3*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     └─systemd-logind
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─mysqld───77*[{mysqld}]
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     ├─systemd-logind
        │                     └─xinetd
        ├─lxc-start───systemd─┬─5*[agetty]
        │                     ├─apt-cacher-ng───8*[{apt-cacher-ng}]
        │                     ├─crond
        │                     ├─dbus-daemon
        │                     ├─dhclient
        │                     ├─nginx───2*[nginx]
        │                     ├─pypi-server───237*[{pypi-server}]
        │                     ├─rsync
        │                     ├─rsyslogd───4*[{rsyslogd}]
        │                     ├─sshd
        │                     ├─systemd-journal
        │                     └─systemd-logind
        ├─master─┬─pickup
        │        └─qmgr
        ├─polkitd───6*[{polkitd}]
        ├─rpcbind
        ├─rsyslogd───5*[{rsyslogd}]
        ├─sshd─┬─sshd───lxc-attach───su───sh───sudo───sh───python───python───neutron-db-mana
        │      └─sshd───sshd───bash───pstree
        ├─systemd-journal
        ├─systemd-logind
        ├─systemd-udevd
        └─tuned───4*[{tuned}]
[vagrant@control ~]$ 


[vagrant@control ~]$ brctl show
bridge name	bridge id		STP enabled	interfaces
br-mgmt		8000.0800277bd9b7	no		0674895c_eth1
							11734601_eth1
							179eba5a_eth1
							1dbe0b7a_eth1
							1de92927_eth1
							1e3d12d9_eth1
							21e6d45f_eth1
							280f6f5a_eth1
							3c65c331_eth1
							3ca87b11_eth1
							47a7eec0_eth1
							5103f3b8_eth1
							6d9d4294_eth1
							758f0659_eth1
							76ec7893_eth1
							7d277f71_eth1
							8de920e2_eth1
							9e87261d_eth1
							a2b512e9_eth1
							af9f1617_eth1
							c34abfbe_eth1
							ed0fcd59_eth1
							eth2
							f34593d8_eth1
							fbd3b559_eth1
br-storage		8000.080027578616	no		179eba5a_eth2
							6d9d4294_eth2
							9e87261d_eth2
							ed0fcd59_eth2
							eth3
br-vlan		8000.08002773a6ad	no		eth5
br-vxlan		8000.08002705c2db	no		eth4
lxcbr0		8000.fe018eacb221	no		0674895c_eth0
							11734601_eth0
							179eba5a_eth0
							1dbe0b7a_eth0
							1de92927_eth0
							1e3d12d9_eth0
							21e6d45f_eth0
							280f6f5a_eth0
							3c65c331_eth0
							3ca87b11_eth0
							47a7eec0_eth0
							5103f3b8_eth0
							6d9d4294_eth0
							758f0659_eth0
							76ec7893_eth0
							7d277f71_eth0
							8de920e2_eth0
							9e87261d_eth0
							a2b512e9_eth0
							af9f1617_eth0
							c34abfbe_eth0
							ed0fcd59_eth0
							f34593d8_eth0
							fbd3b559_eth0
[vagrant@control ~]$ 


[vagrant@control ~]$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0 440320 213348     12 748712    2    9   164   397  425  287 13  3 84  0  0
 2  1 440320 201604     12 751284    0    0  2508  1156 3475 1967 25  7 67  1  0
 2  0 440320 182640     12 752116    0    0   916    26 4149 1773 39  9 52  1  0
 2  0 440320 186440     12 752800    0    0   692    72 3376 1663 33  5 61  1  0
 4  0 440320 184704     12 752812    0    0     0   270 3459 2000 27  6 67  0  0
 2  0 440320 166272     12 753052    0    0    56    53 3814 1554 41  6 52  0  0
 2  0 440320 162864     12 753004    0    0     0  1210 3166 1867 31  5 64  0  0
 2  0 440320 151388     12 754352    0    0  1216    20 3615 1399 36  5 58  0  0
 1  0 440320 161224     12 754052    0    0    80   634 4544 1638 33  7 60  0  0
 1  2 440320 137180     12 761680    0    0  7320     8 3678 1792 28  8 62  3  0
 2  0 443392 137968     12 743380    0 3040  8608  3564 5945 1682 43  9 45  3  0
 2  0 443392 133388     12 745172    0    0  2688     0 3443 1544 38  5 56  1  0
 2  0 443392 149232     12 746232    0    0   872    54 4330 1513 35  7 58  1  0
 2  0 443392 130172     12 746328    0    0   344  1330 4548 2177 32  9 58  0  0
^C
[vagrant@control ~]$ 

30분째 돌고 있다.

* again from 

ssh 로그인도 안된다. control

TASK [rsyslog_client : Write rsyslog config for converting logs into syslog messages] *******************************
Saturday 27 July 2019  01:55:46 +0900 (0:00:09.138)       0:44:58.925 ********* 

virtualbox에서 붙으려 한다.
control login: root
[x.x] Out of memory: Kill proces 14818 (mysqld) score 74 or sacrifice child
[x.x] Killed process 14818 (mysqld) 

top - 02:04:44 up  4:39,  1 user,  load average: 57.54, 100.26, 57.39
Tasks: 625 total,   2 running, 623 sleeping,   0 stopped,   0 zombie
%Cpu(s): 27.2 us,  4.0 sy,  0.0 ni, 64.6 id,  3.0 wa,  0.0 hi,  1.2 si,  0.0 st
KiB Mem :  8008928 total,   126948 free,  7355084 used,   526896 buff/cache
KiB Swap:  1572860 total,        8 free,  1572852 used.   135696 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                              
 3697 neutron   20   0  333168  68112   6408 R  99.7  0.9   0:06.19 neutron-linuxbr     


TASK [Create DB for service] ****************************************************************************************
Saturday 27 July 2019  02:04:08 +0900 (0:00:00.268)       0:53:20.255 ********* 
fatal: [infra1_horizon_container-280f6f5a]: FAILED! => {"censored": "the output has been hidden due to the fact that 'no_log: true' was specified for this result", "changed": false, "failed": true}

PLAY RECAP **********************************************************************************************************
compute1                   : ok=162  changed=66   unreachable=0    failed=0   
infra1                     : ok=71   changed=38   unreachable=0    failed=0   
infra1_cinder_api_container-179eba5a : ok=78   changed=1    unreachable=0    failed=0   
infra1_glance_container-9e87261d : ok=67   changed=2    unreachable=0    failed=0   
infra1_heat_api_container-1e3d12d9 : ok=82   changed=54   unreachable=0    failed=0   
infra1_horizon_container-280f6f5a : ok=10   changed=4    unreachable=0    failed=1   
infra1_keystone_container-3c65c331 : ok=103  changed=6    unreachable=0    failed=0   
infra1_neutron_server_container-47a7eec0 : ok=77   changed=48   unreachable=0    failed=0   
infra1_nova_api_container-758f0659 : ok=95   changed=13   unreachable=0    failed=0   
localhost                  : ok=0    changed=0    unreachable=0    failed=0   
storage1                   : ok=50   changed=1    unreachable=0    failed=0   

Saturday 27 July 2019  02:04:11 +0900 (0:00:02.875)       0:53:23.131 ********* 
=============================================================================== 
rsyslog_client : Write rsyslog config for converting logs into syslog messages ----------------------------- 469.70s
os_neutron : Install neutron role packages ------------------------------------------------------------------ 84.38s
os_neutron : Install neutron role packages ------------------------------------------------------------------ 55.38s
os_neutron : Copy neutron rootwrap filters ------------------------------------------------------------------ 42.18s
os_neutron : Copy neutron rootwrap filters ------------------------------------------------------------------ 36.62s
Execute service action -------------------------------------------------------------------------------------- 35.25s
Ensure rabbitmq user ---------------------------------------------------------------------------------------- 33.62s
os_heat : Install requires pip packages --------------------------------------------------------------------- 31.56s
os_neutron : Install requires pip packages ------------------------------------------------------------------ 29.48s
os_nova : Compile new SELinux policy ------------------------------------------------------------------------ 28.90s
os_nova : Perform a cell_v2 discover ------------------------------------------------------------------------ 27.59s
Perform online data migrations ------------------------------------------------------------------------------ 24.39s
os_nova : Create the cell1 mapping entry in the nova API DB ------------------------------------------------- 23.65s
os_nova : Create the cell0 mapping entry in the nova API DB ------------------------------------------------- 23.47s
os_neutron : Perform a DB expand ---------------------------------------------------------------------------- 23.17s
os_nova : Synchronize the nova DB schema -------------------------------------------------------------------- 22.99s
os_neutron : Unarchive pre-built venv ----------------------------------------------------------------------- 22.84s
os_nova : Synchronize the nova API DB schema ---------------------------------------------------------------- 22.84s
os_heat : Install distro packages --------------------------------------------------------------------------- 22.19s
os_nova : Stop services ------------------------------------------------------------------------------------- 19.86s

EXIT NOTICE [Playbook execution failure] **************************************
===============================================================================
[root@deployer playbooks]# 
[root@deployer playbooks]# 

* TODO finally have to 


lxc-ls | grep utility

lxc-attach -n infra1_utility_container-x

openstack user list --os-cloud=default



Verifying the Dashboard (horizon)¶

    With a web browser, access the Dashboard by using the external load balancer IP address defined by the external_lb_vip_address option in the /etc/openstack_deploy/openstack_user_config.yml file. The Dashboard uses HTTPS on port 443.
    Authenticate by using the admin user name and the password defined by the keystone_auth_admin_password option in the /etc/openstack_deploy/user_secrets.yml file.

* again

- rpmdb broken again

#+BEGIN_SRC 
rm -f /var/lib/rpm/__db.00*
rpm --rebuilddb
#+END_SRC

- since the beginning, openstack-ansible setup-host.yml at playbook

#+BEGIN_SRC 
cd /opt/openstack-ansible/playbook/
openstack-ansible setup-host.yml
#+END_SRC

* with playbook setup-host something

TASK [lxc_hosts : Ensure image has been pre-staged] *****************************************************************************************************************************************************************************************
Monday 29 July 2019  11:12:02 +0900 (0:00:01.081)       0:05:12.154 ***********
fatal: [infra1]: FAILED! => {"ansible_job_id": "559411145086.25886", "attempts": 1, "changed": true, "cmd": "
aria2c --max-connection-per-server=4 --allow-overwrite=true --dir=/tmp --out=rootfs.tar.xz --check-certificate=true https://us.images.linuxcontainers.org/images/centos/7/amd64/default/20190728_07:08/rootfs.tar.xz https://uk.images.linuxcontainers.org/images/centos/7/amd64/default/20190728_07:08/rootfs.tar.xz  > /var/log/aria2c-image-prestage.log 2>&1", "delta": "0:00:01.482925", "end": "2019-07-29 11:10:56.046070", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2019-07-29 11:10:54.563145", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
