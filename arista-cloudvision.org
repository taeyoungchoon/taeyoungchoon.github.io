* on cloudvision

* TODO with cloud vision exchange, cve

- on eos

* with cloud vision portal, cvp

- on kvm using 'Automated CloudVision on KVM Deployment'
- CVP OVA on ESX

* TODO [2022-03-25 금] cvp

https://www.arista.com/en/cg-cv/cv-cvp-virtual-appliance

Hypervisor 	Version
VMware ESX 	5.5
Linux RHEL 	6.5-7.0 

The CVP virtual appliance is a packaged ova file that consists of 
Base OS packages, 
Hadoop, 
HBase, 
Apache Tomcat, 
JAVA jdk 
and the CVP web application. 

https://www.arista.com/en/cg-cv/cv-cvp-cluster-mechanism

CVP consists of distributed components such as 
Zookeeper, 
Hadoop/HDFS and HBase.

https://www.arista.com/en/cg-cv/cv-cv-cue

CV-CUE is containerized within the CV whether it's CVA (CV on a CV appliance) or a standalone CV VM. 

https://www.arista.com/en/cg-cv/cv-deploying-cvp-on-kvm

Deploying CVP on KVM

https://www.arista.com/en/cg-cv/cv-shell-based-configuration

Accept the default or choose a custom “internal cluster network”, for the internal kubernetes clustering.

Running : /bin/sudo /bin/systemctl enable kube-cluster.path
Running : /bin/sudo /bin/systemctl start kube-cluster.path

https://www.arista.com/en/cg-cv/cv-shell-reconfiguration-of-single-node-multi-node-systems

You can change some, but not all of the configuration settings during the reconfiguration. The configuration parameters you cannot change are read-only after the initial configuration.

https://www.arista.com/en/cg-cv/cv-upgrading-cloudvision-portal-cvp

Note: Upgrade to 2021.1.0 and newer requires the configuration of a kubernetes cluster network. 

https://aristanetworks.force.com/AristaCommunity/s/article/cvp-to-k8s

CVP to K8s: full-scale production network simulation

https://www.arista.com/assets/data/pdf/user-manual/um-books/CV_Config_Guide_2021.2.0.pdf

rsync -rtvp 172.31.0.161:/cvpi/tls/certs/kube-cert.pem /cvpi/tls/certs

* histo

eos with nd(network device)
openstack cvx
nsx cvx
cvp
more cloud
* TODO with ztp

https://www.arista.com/en/cg-cv/cv-dhcp-service-for-zero-touch-provisioning-ztp-setup
CloudVision - DHCP Service for Zero Touch Provisioning (ztp) Setup 

The ZTP process relies on a DHCP server to get devices registered with CVP. The DHCP server can be on the CVP, but is more commonly an external DHCP server. 

** doc

https://www.arista.com/en/cg-cv/cv-device-bootstrap-process?searchword=ztp
Device Bootstrap Process 

The URL with IP address will be like this//ipaddress/ztp/bootstrap

https://www.arista.com/en/cg-cv/cv-replacing-switches-using-the-ztr-feature?searchword=ztp
Replacing Switches Using the ZTR Feature

** ansible way

https://avd.sh/en/stable/docs/how-to/ztp.html
ansible some

** people talks

https://aristanetworks.force.com/AristaCommunity/s/question/0D52I00007ERqNXSA1/how-is-ztp-service-hosted-on-cvp-server
The URL to specify is http:// <ip of cvp server>/ztp/bootstrap

kubectl get pod -l app=ztp -o wide

https://aristanetworks.force.com/AristaCommunity/s/question/0D52I00007ERqRZSA1/ztp-bootstrap-script-file
detail

https://aristanetworks.force.com/AristaCommunity/s/article/ztp-boot-process-with-cloudvision
ZTP Boot Process with CloudVision

In this setup, CloudVision ZTP service and the switch are on different IP subnets, 192.168.1.0/24 and 10.1.1.0/24 respectively.  
A Palo Alto FW acts as both a layer 3 routing device and a DHCP server providing a DHCP option 67 to the location of the bootfile.
Option 67 -Bootstrap URL is: http://192.168.1.248/ztp/bootstrap

* DONE remove switch from cvp

https://aristanetworks.force.com/AristaCommunity/s/question/0D52I00007ERqS4SAL/remove-a-switch-from-cvp

https://www.arista.com/en/cg-cv/cv-troubleshooting

** kill terminator to do

- [ ] kill terminator, nop TerminAttr

switch (config)# daemon TerminAttr
switch (config-daemon-TerminAttr)# show active
switch (config-daemon-TerminAttr)# shutdown 

WAIT

log appear:
arista1#reload nowJul 25 00:35:20 arista1 ZeroTouch: %ZTP-4-EXEC_SCRIPT_TIMEOUT: Timed out executing the downloaded config script after 900 seconds
Jul 25 00:35:20 arista1 ZeroTouch: %ZTP-6-RETRY: Retrying Zero Touch Provisioning from the beginning (attempt 1)

** reload with ztp enabled status

- [ ] reload with zerotouch enabled status by that

switch# del startup-config
switch# reload now

** at this time, at the CVP

- [ ] remove from provisioning
- [ ] decommission device

WAIT

** log, hostname from reverse dns lookup record

Jul 25 00:41:37 localhost ZeroTouch: %ZTP-6-DHCPv4_SUCCESS: DHCPv4 response received on Management1  [ Ip Address: 192.168.25.251/24; Nameserver: 192.168.25.211; Domain: t.com; Gateway: 192.168.25.1; Boot File: https://192.168.25.208/ztp/bootstrap ] (option 67)
Jul 25 00:42:02 localhost hostname arista1-by-in-addr ipAddrs ['192.168.25.251']

localhost login: admin
arista1-by-in-addr>

** cvp as arista1-by-in-addr, haha
** configlet-s , ztp part 2 started

1) move
2) and map configlet-s

** so what will be

arista1 login: cvpadmin
Password:
arista1>

* TODO with code

#+BEGIN_SRC python
  hostname = 'leaf1'
  management_ip = '192.168.25.201'
  system_mac_address = 'xx:xx:xx:xx:xx:xx'

  print(f'map {system_mac_address} to {hostname} then provide {management_ip}')
  print("map {} to {} then provide {}".format(system_mac_address, hostname, management_ip))
  print("map %s to %s then provide %s"%(system_mac_address, hostname, management_ip))

  return "map {} to {} then provide {}".format(system_mac_address, hostname, management_ip)
#+END_SRC

#+RESULTS:
: map xx:xx:xx:xx:xx:xx to leaf1 then provide 192.168.25.201

#+BEGIN_SRC python
  nd = [['xx', 'leaf1', '1.1'],
	['yy', 'leaf2', '2.2'],
	['zz', 'leaf3', '3.3']]
  return nd
#+END_SRC

#+RESULTS:
| xx | leaf1 | 1.1 |
| yy | leaf2 | 2.2 |
| zz | leaf3 | 3.3 |

#+BEGIN_SRC python
  kv = {1: [2, 3],
	5: [6, 7]}
  req = 1

  if kv.get(req):
      mc = req
      hn = kv[1][0]
      ip = kv[1][1]
      return "{} {} {}".format(mc, hn, ip)
#+END_SRC

#+RESULTS:
: 1 2 3

#+BEGIN_SRC python
  kv = {'aa': ['leaf1', '1.1'],
	'bb': ['leaf2', '2.2']}

  req = 'aa'

  if kv.get(req):
      mc = req
      hn = kv[req][0]
      ip = kv[req][1]
      output = "map mac address '{}' to hostname '{}' then provide ip '{}'".format(mc, hn, ip)
      print(output)

  return output
#+END_SRC

#+RESULTS:
: map mac address 'aa' to hostname 'leaf1' then provide ip '1.1'

#+BEGIN_SRC python
  network_device = {'aa:bb:cc': ['leaf1', '1.1'],
		    'bb:cc:dd': ['leaf2', '2.2']}

  request = 'bb:cc:dd'

  def lookup(mac_address):
      if network_device.get(mac_address):
	  hostname = network_device[mac_address][0]
	  ip = network_device[mac_address][1]
	  output = "map '{}' to hostname '{}' then provide management ip to '{}'".format(mac_address, hostname, ip)
	  return output

  print(lookup(request))
#+END_SRC

#+RESULTS:
* TODO [2023-02-28 화] spec requirement

- https://www.arista.com/en/cg-cv/cv-system-requirements
- 무겁다
- 그건 변하지 않는다
- 변할 수 있다
- [ ] latency requirements가 생겼다. 예전에는 보지 못했던 것이다. 기억의 오류일까.
  - cvp node와 eos device의 latency는 500 ms 이하여야 한다.
  - 물리 어플라이언스 장비는 하나의 랙에 두기를 권고한다.
    - 아니더라도 latency는 충분히 보장되어야 한다.
    - 가상 어플라이언스 장비도 마찬가지다.
  
** lab deployment

- cpu 16 unit: Core
- ram 32 unit: GB
- dsk  1 unit: TB
- dsk.throughput 20 unit: MB/s

** production deployment

- cpu 28 unit: Core
- ram 52 unit: GB
- dsk  1 unit: TB
- dsk.throughput 40 unit: MB/s

#+BEGIN_SRC 
python generateXmlForKvm.py -n cvp1 --device-bridge br0 -k 5 -i cvpTemplate.xml -x disk1.qcow2 -y disk2.qcow2 -b 53248 -p 28 -e /usr/libexec/qemu-kvm -o cvp1.xml

#+END_SRC

* TODO [2023-02-28 화] simply as monitor, yes

but how

* DONE cvp version-s

** list

- 2021.3.0
  - will using but don't
- 2023.1.2
  - bug
- [ ] 2022.2.0
  - stable
  - used at somewhere
- 2021.3.0
  - lab
    - static analyze
      - CloudVision Portal 2021.3.0

* DONE upgrade from 2021.3.0 to 2022.2.0

- [X] how to upgrade from ver to ver

** steps

1) SSH as root into the primary node.
2) Run these commands:
   #+begin_example
     rm -rf /tmp/upgrade
     mkdir /data/upgrade
     ln -s /data/upgrade /tmp/upgrade
     SCP cvp-upgrade-2022.2.0.tgz /data/upgrade
   #+end_example
3) Run the 'su cvpadmin' command to trigger the shell.
   - Select the upgrade option from the shell.

** note

- Upgrading to the current version may take up to 30 minutes.
